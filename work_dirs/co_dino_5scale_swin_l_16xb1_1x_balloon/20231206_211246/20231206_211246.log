2023/12/06 21:12:46 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
    CUDA available: True
    numpy_random_seed: 1128920705
    GPU 0: NVIDIA GeForce RTX 3060
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.8, V11.8.89
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.1.1
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.1
    OpenCV: 4.8.1
    MMEngine: 0.10.1

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1128920705
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2023/12/06 21:12:47 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=16, enable=False)
backend_args = None
batch_augments = [
    dict(pad_mask=True, size=(
        1024,
        1024,
    ), type='BatchFixedSizePad'),
]
custom_imports = dict(
    allow_failed_imports=False, imports=[
        'projects.CO-DETR.codetr',
    ])
data_root = 'data/balloon/'
dataset_type = 'CocoDataset'
default_hooks = dict(
    checkpoint=dict(
        _scope_='mmdet',
        by_epoch=True,
        interval=1,
        save_best='coco/bbox_mAP',
        type='CheckpointHook'),
    logger=dict(_scope_='mmdet', interval=50, type='LoggerHook'),
    param_scheduler=dict(_scope_='mmdet', type='ParamSchedulerHook'),
    sampler_seed=dict(_scope_='mmdet', type='DistSamplerSeedHook'),
    timer=dict(_scope_='mmdet', type='IterTimerHook'),
    visualization=dict(_scope_='mmdet', type='DetVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
image_size = (
    1024,
    1024,
)
launcher = 'none'
load_from = 'https://download.openmmlab.com/mmdetection/v3.0/codetr/co_dino_5scale_swin_large_1x_coco-27c13da4.pth'
load_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.1,
            2.0,
        ),
        scale=(
            1024,
            1024,
        ),
        type='RandomResize'),
    dict(
        allow_negative_crop=True,
        crop_size=(
            1024,
            1024,
        ),
        crop_type='absolute_range',
        recompute_bbox=True,
        type='RandomCrop'),
    dict(min_gt_bbox_wh=(
        0.01,
        0.01,
    ), type='FilterAnnotations'),
    dict(prob=0.5, type='RandomFlip'),
    dict(pad_val=dict(img=(
        114,
        114,
        114,
    )), size=(
        1024,
        1024,
    ), type='Pad'),
]
log_level = 'INFO'
log_processor = dict(
    _scope_='mmdet', by_epoch=True, type='LogProcessor', window_size=50)
loss_lambda = 2.0
max_epochs = 12
max_iters = 270000
metainfo = dict(
    classes=('balloon', ), palette=[
        (
            220,
            20,
            60,
        ),
    ])
model = dict(
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=192,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            6,
            12,
            24,
            48,
        ],
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        patch_norm=True,
        pretrain_img_size=384,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=12,
        with_cp=True),
    bbox_head=[
        dict(
            anchor_generator=dict(
                octave_base_scale=8,
                ratios=[
                    1.0,
                ],
                scales_per_octave=1,
                strides=[
                    4,
                    8,
                    16,
                    32,
                    64,
                    128,
                ],
                type='AnchorGenerator'),
            bbox_coder=dict(
                target_means=[
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                ],
                target_stds=[
                    0.1,
                    0.1,
                    0.2,
                    0.2,
                ],
                type='DeltaXYWHBBoxCoder'),
            feat_channels=256,
            in_channels=256,
            loss_bbox=dict(loss_weight=24.0, type='GIoULoss'),
            loss_centerness=dict(
                loss_weight=12.0, type='CrossEntropyLoss', use_sigmoid=True),
            loss_cls=dict(
                alpha=0.25,
                gamma=2.0,
                loss_weight=12.0,
                type='FocalLoss',
                use_sigmoid=True),
            num_classes=1,
            stacked_convs=1,
            type='CoATSSHead'),
    ],
    data_preprocessor=dict(
        batch_augments=None,
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    eval_module='detr',
    neck=dict(
        act_cfg=None,
        in_channels=[
            192,
            384,
            768,
            1536,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=5,
        out_channels=256,
        type='ChannelMapper'),
    query_head=dict(
        as_two_stage=True,
        dn_cfg=dict(
            box_noise_scale=1.0,
            group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
            label_noise_scale=0.5),
        in_channels=2048,
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            beta=2.0,
            loss_weight=1.0,
            type='QualityFocalLoss',
            use_sigmoid=True),
        loss_iou=dict(loss_weight=2.0, type='GIoULoss'),
        num_classes=1,
        num_query=900,
        positional_encoding=dict(
            normalize=True,
            num_feats=128,
            temperature=20,
            type='SinePositionalEncoding'),
        transformer=dict(
            decoder=dict(
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    attn_cfgs=[
                        dict(
                            dropout=0.0,
                            embed_dims=256,
                            num_heads=8,
                            type='MultiheadAttention'),
                        dict(
                            dropout=0.0,
                            embed_dims=256,
                            num_levels=5,
                            type='MultiScaleDeformableAttention'),
                    ],
                    feedforward_channels=2048,
                    ffn_dropout=0.0,
                    operation_order=(
                        'self_attn',
                        'norm',
                        'cross_attn',
                        'norm',
                        'ffn',
                        'norm',
                    ),
                    type='DetrTransformerDecoderLayer'),
                type='DinoTransformerDecoder'),
            encoder=dict(
                num_layers=6,
                transformerlayers=dict(
                    attn_cfgs=dict(
                        dropout=0.0,
                        embed_dims=256,
                        num_levels=5,
                        type='MultiScaleDeformableAttention'),
                    feedforward_channels=2048,
                    ffn_dropout=0.0,
                    operation_order=(
                        'self_attn',
                        'norm',
                        'ffn',
                        'norm',
                    ),
                    type='BaseTransformerLayer'),
                type='DetrTransformerEncoder',
                with_cp=6),
            num_co_heads=2,
            num_feature_levels=5,
            type='CoDinoTransformer',
            with_coord_feat=False),
        type='CoDINOHead'),
    roi_head=[
        dict(
            bbox_head=dict(
                bbox_coder=dict(
                    target_means=[
                        0.0,
                        0.0,
                        0.0,
                        0.0,
                    ],
                    target_stds=[
                        0.1,
                        0.1,
                        0.2,
                        0.2,
                    ],
                    type='DeltaXYWHBBoxCoder'),
                fc_out_channels=1024,
                in_channels=256,
                loss_bbox=dict(loss_weight=120.0, type='GIoULoss'),
                loss_cls=dict(
                    loss_weight=12.0,
                    type='CrossEntropyLoss',
                    use_sigmoid=False),
                num_classes=1,
                reg_class_agnostic=False,
                reg_decoded_bbox=True,
                roi_feat_size=7,
                type='Shared2FCBBoxHead'),
            bbox_roi_extractor=dict(
                featmap_strides=[
                    4,
                    8,
                    16,
                    32,
                    64,
                ],
                finest_scale=56,
                out_channels=256,
                roi_layer=dict(
                    output_size=7, sampling_ratio=0, type='RoIAlign'),
                type='SingleRoIExtractor'),
            type='CoStandardRoIHead'),
    ],
    rpn_head=dict(
        anchor_generator=dict(
            octave_base_scale=4,
            ratios=[
                0.5,
                1.0,
                2.0,
            ],
            scales_per_octave=3,
            strides=[
                4,
                8,
                16,
                32,
                64,
                128,
            ],
            type='AnchorGenerator'),
        bbox_coder=dict(
            target_means=[
                0.0,
                0.0,
                0.0,
                0.0,
            ],
            target_stds=[
                1.0,
                1.0,
                1.0,
                1.0,
            ],
            type='DeltaXYWHBBoxCoder'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=12.0, type='L1Loss'),
        loss_cls=dict(
            loss_weight=12.0, type='CrossEntropyLoss', use_sigmoid=True),
        type='RPNHead'),
    test_cfg=[
        dict(max_per_img=300, nms=dict(iou_threshold=0.8, type='soft_nms')),
        dict(
            rcnn=dict(
                max_per_img=100,
                nms=dict(iou_threshold=0.5, type='nms'),
                score_thr=0.0),
            rpn=dict(
                max_per_img=1000,
                min_bbox_size=0,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=1000)),
        dict(
            max_per_img=100,
            min_bbox_size=0,
            nms=dict(iou_threshold=0.6, type='nms'),
            nms_pre=1000,
            score_thr=0.0),
    ],
    train_cfg=[
        dict(
            assigner=dict(
                match_costs=[
                    dict(type='FocalLossCost', weight=2.0),
                    dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                    dict(iou_mode='giou', type='IoUCost', weight=2.0),
                ],
                type='HungarianAssigner')),
        dict(
            rcnn=dict(
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=False,
                    min_pos_iou=0.5,
                    neg_iou_thr=0.5,
                    pos_iou_thr=0.5,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=True,
                    neg_pos_ub=-1,
                    num=512,
                    pos_fraction=0.25,
                    type='RandomSampler')),
            rpn=dict(
                allowed_border=-1,
                assigner=dict(
                    ignore_iof_thr=-1,
                    match_low_quality=True,
                    min_pos_iou=0.3,
                    neg_iou_thr=0.3,
                    pos_iou_thr=0.7,
                    type='MaxIoUAssigner'),
                debug=False,
                pos_weight=-1,
                sampler=dict(
                    add_gt_as_proposals=False,
                    neg_pos_ub=-1,
                    num=256,
                    pos_fraction=0.5,
                    type='RandomSampler')),
            rpn_proposal=dict(
                max_per_img=1000,
                min_bbox_size=0,
                nms=dict(iou_threshold=0.7, type='nms'),
                nms_pre=4000)),
        dict(
            allowed_border=-1,
            assigner=dict(topk=9, type='ATSSAssigner'),
            debug=False,
            pos_weight=-1),
    ],
    type='CoDETR',
    use_lsj=False)
num_classes = 1
num_dec_layer = 6
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0002, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(custom_keys=dict(backbone=dict(lr_mult=0.1))),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=12,
        gamma=0.1,
        milestones=[
            11,
        ],
        type='MultiStepLR'),
]
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'
resume = False
test_cfg = dict(_scope_='mmdet', type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        _scope_='mmdet',
        ann_file='val/annotation_coco.json',
        backend_args=None,
        data_prefix=dict(img='val/'),
        data_root='data/balloon/',
        metainfo=dict(classes=('balloon', ), palette=[
            (
                220,
                20,
                60,
            ),
        ]),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                1333,
                800,
            ), type='Resize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='CocoDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(_scope_='mmdet', shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    _scope_='mmdet',
    ann_file='data/balloon/val/annotation_coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        1333,
        800,
    ), type='Resize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='train/annotation_coco.json',
        backend_args=None,
        data_prefix=dict(img='train/'),
        data_root='data/balloon/',
        filter_cfg=dict(filter_empty_gt=False, min_size=32),
        metainfo=dict(classes=('balloon', ), palette=[
            (
                220,
                20,
                60,
            ),
        ]),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    400,
                                    4200,
                                ),
                                (
                                    500,
                                    4200,
                                ),
                                (
                                    600,
                                    4200,
                                ),
                            ],
                            type='RandomChoiceResize'),
                        dict(
                            allow_negative_crop=True,
                            crop_size=(
                                384,
                                600,
                            ),
                            crop_type='absolute_range',
                            type='RandomCrop'),
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                ],
                type='RandomChoice'),
            dict(type='PackDetInputs'),
        ],
        type='CocoDataset'),
    num_workers=1,
    persistent_workers=True,
    sampler=dict(_scope_='mmdet', shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            400,
                            4200,
                        ),
                        (
                            500,
                            4200,
                        ),
                        (
                            600,
                            4200,
                        ),
                    ],
                    type='RandomChoiceResize'),
                dict(
                    allow_negative_crop=True,
                    crop_size=(
                        384,
                        600,
                    ),
                    crop_type='absolute_range',
                    type='RandomCrop'),
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
        ],
        type='RandomChoice'),
    dict(type='PackDetInputs'),
]
val_cfg = dict(_scope_='mmdet', type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        _scope_='mmdet',
        ann_file='val/annotation_coco.json',
        backend_args=None,
        data_prefix=dict(img='val/'),
        data_root='data/balloon/',
        metainfo=dict(classes=('balloon', ), palette=[
            (
                220,
                20,
                60,
            ),
        ]),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                1333,
                800,
            ), type='Resize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetInputs'),
        ],
        test_mode=True,
        type='CocoDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(_scope_='mmdet', shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    _scope_='mmdet',
    ann_file='data/balloon/val/annotation_coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
vis_backends = [
    dict(_scope_='mmdet', type='LocalVisBackend'),
]
visualizer = dict(
    _scope_='mmdet',
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/co_dino_5scale_swin_l_16xb1_1x_balloon'

Name of parameter - Initialization information

rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_cls.weight - torch.Size([9, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_cls.bias - torch.Size([9]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_reg.weight - torch.Size([36, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_reg.bias - torch.Size([36]): 
NormalInit: mean=0, std=0.01, bias=0 
Name of parameter - Initialization information

bbox_head.fc_cls.weight - torch.Size([2, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.fc_cls.bias - torch.Size([2]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.fc_reg.weight - torch.Size([4, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

bbox_head.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 
Name of parameter - Initialization information

cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

cls_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

cls_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

reg_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

reg_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

atss_cls.weight - torch.Size([1, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

atss_cls.bias - torch.Size([1]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

atss_reg.weight - torch.Size([4, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

atss_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.01, bias=0 

atss_centerness.weight - torch.Size([1, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

atss_centerness.bias - torch.Size([1]): 
NormalInit: mean=0, std=0.01, bias=0 

scales.0.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

scales.1.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

scales.2.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

scales.3.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

scales.4.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoATSSHead  

scales.5.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoATSSHead  
2023/12/06 21:12:49 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2023/12/06 21:12:49 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=2e-05
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
2023/12/06 21:12:51 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
2023/12/06 21:12:52 - mmengine - INFO - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([192, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([529, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([1536, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 48]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 48]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm0.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm3.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of CoDETR  

backbone.norm3.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.0.conv.weight - torch.Size([256, 192, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.1.conv.weight - torch.Size([256, 384, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.2.conv.weight - torch.Size([256, 768, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.3.conv.weight - torch.Size([256, 1536, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.3.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.convs.3.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.extra_convs.0.conv.weight - torch.Size([256, 1536, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.extra_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

neck.extra_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.level_embeds - torch.Size([5, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([320, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([160, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.ref_point_head.0.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.ref_point_head.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.ref_point_head.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.ref_point_head.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.decoder.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.enc_output.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.enc_output.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.enc_output_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.enc_output_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.query_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans.0.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans.1.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.transformer.aux_pos_trans_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.0.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.1.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.2.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.3.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.3.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.5.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.5.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.6.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.cls_branches.6.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.0.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.1.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.2.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.3.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.4.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.5.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.6.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.6.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.6.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.6.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.6.4.weight - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.reg_branches.6.4.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.downsample.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.downsample.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

query_head.dn_generator.label_embedding.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of CoDETR  

rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

rpn_head.rpn_conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

rpn_head.rpn_cls.weight - torch.Size([9, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of CoDETR  

rpn_head.rpn_cls.bias - torch.Size([9]): 
The value is the same before and after calling `init_weights` of CoDETR  

rpn_head.rpn_reg.weight - torch.Size([36, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of CoDETR  

rpn_head.rpn_reg.bias - torch.Size([36]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.fc_cls.weight - torch.Size([2, 1024]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.fc_cls.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.fc_reg.weight - torch.Size([4, 1024]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.fc_reg.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of CoDETR  

roi_head.0.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.cls_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.cls_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.reg_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.reg_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.atss_cls.weight - torch.Size([1, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.atss_cls.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.atss_reg.weight - torch.Size([4, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.atss_reg.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.atss_centerness.weight - torch.Size([1, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.atss_centerness.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.scales.0.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.scales.1.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.scales.2.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.scales.3.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.scales.4.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoDETR  

bbox_head.0.scales.5.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of CoDETR  
2023/12/06 21:12:57 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v3.0/codetr/co_dino_5scale_swin_large_1x_coco-27c13da4.pth
2023/12/06 21:12:57 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2023/12/06 21:12:57 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2023/12/06 21:12:57 - mmengine - INFO - Checkpoints will be saved to /home/hinata/mmdetection/work_dirs/co_dino_5scale_swin_l_16xb1_1x_balloon.
2023/12/06 21:15:28 - mmengine - INFO - Epoch(train)  [1][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:34:13  time: 3.0117  data_time: 0.0051  memory: 9765  grad_norm: 290.0143  loss: 54.3076  loss_cls: 0.6980  loss_bbox: 0.4324  loss_iou: 0.3929  d0.loss_cls: 0.9325  d0.loss_bbox: 0.4668  d0.loss_iou: 0.4665  d1.loss_cls: 0.6756  d1.loss_bbox: 0.4240  d1.loss_iou: 0.4164  d2.loss_cls: 0.7730  d2.loss_bbox: 0.4149  d2.loss_iou: 0.4019  d3.loss_cls: 0.6395  d3.loss_bbox: 0.4350  d3.loss_iou: 0.3946  d4.loss_cls: 0.6318  d4.loss_bbox: 0.4343  d4.loss_iou: 0.3913  enc_loss_cls: 0.9329  enc_loss_bbox: 0.5062  enc_loss_iou: 0.5512  dn_loss_cls: 0.4207  dn_loss_bbox: 0.3970  dn_loss_iou: 0.3867  d0.dn_loss_cls: 0.5422  d0.dn_loss_bbox: 0.5600  d0.dn_loss_iou: 0.6517  d1.dn_loss_cls: 0.4031  d1.dn_loss_bbox: 0.4183  d1.dn_loss_iou: 0.4570  d2.dn_loss_cls: 0.4305  d2.dn_loss_bbox: 0.3944  d2.dn_loss_iou: 0.3972  d3.dn_loss_cls: 0.3918  d3.dn_loss_bbox: 0.3968  d3.dn_loss_iou: 0.3865  d4.dn_loss_cls: 0.3965  d4.dn_loss_bbox: 0.3969  d4.dn_loss_iou: 0.3860  loss_rpn_cls: 0.1875  loss_rpn_bbox: 0.0728  loss_cls0: 3.2065  acc0: 98.2422  loss_bbox0: 4.6024  loss_cls1: 8.8722  loss_bbox1: 2.6439  loss_centerness1: 6.9350  loss_cls_aux0: 0.3608  loss_bbox_aux0: 0.0931  loss_iou_aux0: 0.1184  d0.loss_cls_aux0: 0.5827  d0.loss_bbox_aux0: 0.1885  d0.loss_iou_aux0: 0.2444  d1.loss_cls_aux0: 0.4880  d1.loss_bbox_aux0: 0.1154  d1.loss_iou_aux0: 0.1538  d2.loss_cls_aux0: 0.4394  d2.loss_bbox_aux0: 0.0954  d2.loss_iou_aux0: 0.1201  d3.loss_cls_aux0: 0.4298  d3.loss_bbox_aux0: 0.0920  d3.loss_iou_aux0: 0.1164  d4.loss_cls_aux0: 0.3584  d4.loss_bbox_aux0: 0.0928  d4.loss_iou_aux0: 0.1178  loss_cls_aux1: 0.3296  loss_bbox_aux1: 0.1254  loss_iou_aux1: 0.1438  d0.loss_cls_aux1: 0.6070  d0.loss_bbox_aux1: 0.2399  d0.loss_iou_aux1: 0.2912  d1.loss_cls_aux1: 0.3933  d1.loss_bbox_aux1: 0.1480  d1.loss_iou_aux1: 0.1730  d2.loss_cls_aux1: 0.3788  d2.loss_bbox_aux1: 0.1242  d2.loss_iou_aux1: 0.1415  d3.loss_cls_aux1: 0.3967  d3.loss_bbox_aux1: 0.1247  d3.loss_iou_aux1: 0.1421  d4.loss_cls_aux1: 0.3273  d4.loss_bbox_aux1: 0.1252  d4.loss_iou_aux1: 0.1431
2023/12/06 21:16:00 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:16:00 - mmengine - INFO - Saving checkpoint at 1 epochs
2023/12/06 21:16:59 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:16:59 - mmengine - INFO - bbox_mAP_copypaste: 0.821 0.948 0.926 0.435 0.790 0.851
2023/12/06 21:16:59 - mmengine - INFO - Epoch(val) [1][13/13]    coco/bbox_mAP: 0.8210  coco/bbox_mAP_50: 0.9480  coco/bbox_mAP_75: 0.9260  coco/bbox_mAP_s: 0.4350  coco/bbox_mAP_m: 0.7900  coco/bbox_mAP_l: 0.8510  data_time: 0.5038  time: 1.2927
2023/12/06 21:17:02 - mmengine - INFO - The best checkpoint with 0.8210 coco/bbox_mAP at 1 epoch is saved to best_coco_bbox_mAP_epoch_1.pth.
2023/12/06 21:20:03 - mmengine - INFO - Epoch(train)  [2][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:31:02  time: 3.0008  data_time: 0.0098  memory: 10316  grad_norm: 234.4958  loss: 23.5400  loss_cls: 0.1316  loss_bbox: 0.1259  loss_iou: 0.1518  d0.loss_cls: 0.3261  d0.loss_bbox: 0.1524  d0.loss_iou: 0.1860  d1.loss_cls: 0.2131  d1.loss_bbox: 0.1339  d1.loss_iou: 0.1634  d2.loss_cls: 0.1624  d2.loss_bbox: 0.1261  d2.loss_iou: 0.1524  d3.loss_cls: 0.1385  d3.loss_bbox: 0.1150  d3.loss_iou: 0.1530  d4.loss_cls: 0.1349  d4.loss_bbox: 0.1254  d4.loss_iou: 0.1509  enc_loss_cls: 0.2373  enc_loss_bbox: 0.2366  enc_loss_iou: 0.3071  dn_loss_cls: 0.0457  dn_loss_bbox: 0.1047  dn_loss_iou: 0.1540  d0.dn_loss_cls: 0.0960  d0.dn_loss_bbox: 0.2553  d0.dn_loss_iou: 0.3530  d1.dn_loss_cls: 0.0471  d1.dn_loss_bbox: 0.1332  d1.dn_loss_iou: 0.1971  d2.dn_loss_cls: 0.0452  d2.dn_loss_bbox: 0.1090  d2.dn_loss_iou: 0.1605  d3.dn_loss_cls: 0.0455  d3.dn_loss_bbox: 0.1047  d3.dn_loss_iou: 0.1535  d4.dn_loss_cls: 0.0457  d4.dn_loss_bbox: 0.1045  d4.dn_loss_iou: 0.1535  loss_rpn_cls: 0.0208  loss_rpn_bbox: 0.0640  loss_cls0: 0.9614  acc0: 94.1406  loss_bbox0: 2.2427  loss_cls1: 2.1335  loss_bbox1: 2.2770  loss_centerness1: 6.8871  loss_cls_aux0: 0.0424  loss_bbox_aux0: 0.0442  loss_iou_aux0: 0.0714  d0.loss_cls_aux0: 0.0872  d0.loss_bbox_aux0: 0.1477  d0.loss_iou_aux0: 0.2037  d1.loss_cls_aux0: 0.0613  d1.loss_bbox_aux0: 0.0669  d1.loss_iou_aux0: 0.1067  d2.loss_cls_aux0: 0.0317  d2.loss_bbox_aux0: 0.0457  d2.loss_iou_aux0: 0.0728  d3.loss_cls_aux0: 0.0339  d3.loss_bbox_aux0: 0.0436  d3.loss_iou_aux0: 0.0691  d4.loss_cls_aux0: 0.0339  d4.loss_bbox_aux0: 0.0436  d4.loss_iou_aux0: 0.0701  loss_cls_aux1: 0.0077  loss_bbox_aux1: 0.0991  loss_iou_aux1: 0.1266  d0.loss_cls_aux1: 0.0327  d0.loss_bbox_aux1: 0.2000  d0.loss_iou_aux1: 0.2598  d1.loss_cls_aux1: 0.0091  d1.loss_bbox_aux1: 0.1356  d1.loss_iou_aux1: 0.1647  d2.loss_cls_aux1: 0.0126  d2.loss_bbox_aux1: 0.1019  d2.loss_iou_aux1: 0.1309  d3.loss_cls_aux1: 0.0093  d3.loss_bbox_aux1: 0.0987  d3.loss_iou_aux1: 0.1252  d4.loss_cls_aux1: 0.0075  d4.loss_bbox_aux1: 0.0986  d4.loss_iou_aux1: 0.1259
2023/12/06 21:20:35 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:20:35 - mmengine - INFO - Saving checkpoint at 2 epochs
2023/12/06 21:21:01 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:21:01 - mmengine - INFO - bbox_mAP_copypaste: 0.845 0.944 0.925 0.288 0.778 0.883
2023/12/06 21:21:01 - mmengine - INFO - Epoch(val) [2][13/13]    coco/bbox_mAP: 0.8450  coco/bbox_mAP_50: 0.9440  coco/bbox_mAP_75: 0.9250  coco/bbox_mAP_s: 0.2880  coco/bbox_mAP_m: 0.7780  coco/bbox_mAP_l: 0.8830  data_time: 0.0037  time: 0.6439
2023/12/06 21:21:01 - mmengine - INFO - The previous best checkpoint /home/hinata/mmdetection/work_dirs/co_dino_5scale_swin_l_16xb1_1x_balloon/best_coco_bbox_mAP_epoch_1.pth is removed
2023/12/06 21:21:03 - mmengine - INFO - The best checkpoint with 0.8450 coco/bbox_mAP at 2 epoch is saved to best_coco_bbox_mAP_epoch_2.pth.
2023/12/06 21:23:51 - mmengine - INFO - Epoch(train)  [3][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:27:28  time: 2.8426  data_time: 0.0060  memory: 9833  grad_norm: 197.4874  loss: 19.5690  loss_cls: 0.0586  loss_bbox: 0.0724  loss_iou: 0.1076  d0.loss_cls: 0.1968  d0.loss_bbox: 0.1156  d0.loss_iou: 0.1683  d1.loss_cls: 0.1060  d1.loss_bbox: 0.0846  d1.loss_iou: 0.1261  d2.loss_cls: 0.0676  d2.loss_bbox: 0.0734  d2.loss_iou: 0.1096  d3.loss_cls: 0.0709  d3.loss_bbox: 0.0714  d3.loss_iou: 0.1064  d4.loss_cls: 0.0604  d4.loss_bbox: 0.0723  d4.loss_iou: 0.1072  enc_loss_cls: 0.1630  enc_loss_bbox: 0.2122  enc_loss_iou: 0.2910  dn_loss_cls: 0.0208  dn_loss_bbox: 0.0935  dn_loss_iou: 0.1312  d0.dn_loss_cls: 0.0617  d0.dn_loss_bbox: 0.2700  d0.dn_loss_iou: 0.3641  d1.dn_loss_cls: 0.0301  d1.dn_loss_bbox: 0.1361  d1.dn_loss_iou: 0.1885  d2.dn_loss_cls: 0.0254  d2.dn_loss_bbox: 0.0996  d2.dn_loss_iou: 0.1372  d3.dn_loss_cls: 0.0217  d3.dn_loss_bbox: 0.0932  d3.dn_loss_iou: 0.1306  d4.dn_loss_cls: 0.0207  d4.dn_loss_bbox: 0.0933  d4.dn_loss_iou: 0.1306  loss_rpn_cls: 0.0306  loss_rpn_bbox: 0.0496  loss_cls0: 0.6652  acc0: 95.8984  loss_bbox0: 1.2971  loss_cls1: 1.5234  loss_bbox1: 1.9689  loss_centerness1: 6.9915  loss_cls_aux0: 0.0007  loss_bbox_aux0: 0.0420  loss_iou_aux0: 0.0711  d0.loss_cls_aux0: 0.0099  d0.loss_bbox_aux0: 0.1687  d0.loss_iou_aux0: 0.2274  d1.loss_cls_aux0: 0.0083  d1.loss_bbox_aux0: 0.0786  d1.loss_iou_aux0: 0.1175  d2.loss_cls_aux0: 0.0036  d2.loss_bbox_aux0: 0.0459  d2.loss_iou_aux0: 0.0747  d3.loss_cls_aux0: 0.0018  d3.loss_bbox_aux0: 0.0412  d3.loss_iou_aux0: 0.0693  d4.loss_cls_aux0: 0.0009  d4.loss_bbox_aux0: 0.0416  d4.loss_iou_aux0: 0.0702  loss_cls_aux1: 0.0010  loss_bbox_aux1: 0.0766  loss_iou_aux1: 0.1099  d0.loss_cls_aux1: 0.0088  d0.loss_bbox_aux1: 0.1939  d0.loss_iou_aux1: 0.2687  d1.loss_cls_aux1: 0.0051  d1.loss_bbox_aux1: 0.1007  d1.loss_iou_aux1: 0.1457  d2.loss_cls_aux1: 0.0046  d2.loss_bbox_aux1: 0.0787  d2.loss_iou_aux1: 0.1123  d3.loss_cls_aux1: 0.0019  d3.loss_bbox_aux1: 0.0761  d3.loss_iou_aux1: 0.1088  d4.loss_cls_aux1: 0.0012  d4.loss_bbox_aux1: 0.0764  d4.loss_iou_aux1: 0.1093
2023/12/06 21:24:22 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:24:22 - mmengine - INFO - Saving checkpoint at 3 epochs
2023/12/06 21:24:48 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:24:48 - mmengine - INFO - bbox_mAP_copypaste: 0.838 0.943 0.885 0.263 0.777 0.876
2023/12/06 21:24:48 - mmengine - INFO - Epoch(val) [3][13/13]    coco/bbox_mAP: 0.8380  coco/bbox_mAP_50: 0.9430  coco/bbox_mAP_75: 0.8850  coco/bbox_mAP_s: 0.2630  coco/bbox_mAP_m: 0.7770  coco/bbox_mAP_l: 0.8760  data_time: 0.0037  time: 0.6432
2023/12/06 21:27:14 - mmengine - INFO - Epoch(train)  [4][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:24:21  time: 2.9017  data_time: 0.0046  memory: 10681  grad_norm: 239.4754  loss: 18.2376  loss_cls: 0.1079  loss_bbox: 0.0707  loss_iou: 0.0927  d0.loss_cls: 0.2007  d0.loss_bbox: 0.1037  d0.loss_iou: 0.1437  d1.loss_cls: 0.1399  d1.loss_bbox: 0.0752  d1.loss_iou: 0.0992  d2.loss_cls: 0.1271  d2.loss_bbox: 0.0700  d2.loss_iou: 0.0927  d3.loss_cls: 0.1316  d3.loss_bbox: 0.0703  d3.loss_iou: 0.0917  d4.loss_cls: 0.1194  d4.loss_bbox: 0.0704  d4.loss_iou: 0.0922  enc_loss_cls: 0.1517  enc_loss_bbox: 0.1984  enc_loss_iou: 0.2247  dn_loss_cls: 0.0232  dn_loss_bbox: 0.0912  dn_loss_iou: 0.1174  d0.dn_loss_cls: 0.0606  d0.dn_loss_bbox: 0.2446  d0.dn_loss_iou: 0.3069  d1.dn_loss_cls: 0.0323  d1.dn_loss_bbox: 0.1214  d1.dn_loss_iou: 0.1506  d2.dn_loss_cls: 0.0257  d2.dn_loss_bbox: 0.0927  d2.dn_loss_iou: 0.1196  d3.dn_loss_cls: 0.0234  d3.dn_loss_bbox: 0.0908  d3.dn_loss_iou: 0.1166  d4.dn_loss_cls: 0.0235  d4.dn_loss_bbox: 0.0910  d4.dn_loss_iou: 0.1170  loss_rpn_cls: 0.0108  loss_rpn_bbox: 0.0596  loss_cls0: 0.6270  acc0: 99.2188  loss_bbox0: 1.2236  loss_cls1: 1.1439  loss_bbox1: 1.7446  loss_centerness1: 6.8049  loss_cls_aux0: 0.0009  loss_bbox_aux0: 0.0450  loss_iou_aux0: 0.0641  d0.loss_cls_aux0: 0.0292  d0.loss_bbox_aux0: 0.1488  d0.loss_iou_aux0: 0.1942  d1.loss_cls_aux0: 0.0207  d1.loss_bbox_aux0: 0.0681  d1.loss_iou_aux0: 0.0903  d2.loss_cls_aux0: 0.0094  d2.loss_bbox_aux0: 0.0458  d2.loss_iou_aux0: 0.0655  d3.loss_cls_aux0: 0.0025  d3.loss_bbox_aux0: 0.0446  d3.loss_iou_aux0: 0.0630  d4.loss_cls_aux0: 0.0012  d4.loss_bbox_aux0: 0.0448  d4.loss_iou_aux0: 0.0635  loss_cls_aux1: 0.0009  loss_bbox_aux1: 0.0675  loss_iou_aux1: 0.0897  d0.loss_cls_aux1: 0.0179  d0.loss_bbox_aux1: 0.1968  d0.loss_iou_aux1: 0.2379  d1.loss_cls_aux1: 0.0092  d1.loss_bbox_aux1: 0.0895  d1.loss_iou_aux1: 0.1132  d2.loss_cls_aux1: 0.0054  d2.loss_bbox_aux1: 0.0667  d2.loss_iou_aux1: 0.0894  d3.loss_cls_aux1: 0.0024  d3.loss_bbox_aux1: 0.0666  d3.loss_iou_aux1: 0.0883  d4.loss_cls_aux1: 0.0009  d4.loss_bbox_aux1: 0.0673  d4.loss_iou_aux1: 0.0893
2023/12/06 21:27:45 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:27:45 - mmengine - INFO - Saving checkpoint at 4 epochs
2023/12/06 21:28:14 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:28:14 - mmengine - INFO - bbox_mAP_copypaste: 0.844 0.945 0.920 0.423 0.697 0.887
2023/12/06 21:28:14 - mmengine - INFO - Epoch(val) [4][13/13]    coco/bbox_mAP: 0.8440  coco/bbox_mAP_50: 0.9450  coco/bbox_mAP_75: 0.9200  coco/bbox_mAP_s: 0.4230  coco/bbox_mAP_m: 0.6970  coco/bbox_mAP_l: 0.8870  data_time: 0.0060  time: 0.6464
2023/12/06 21:30:32 - mmengine - INFO - Epoch(train)  [5][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:21:09  time: 2.7592  data_time: 0.0050  memory: 10395  grad_norm: 188.6604  loss: 17.8585  loss_cls: 0.0354  loss_bbox: 0.0725  loss_iou: 0.0974  d0.loss_cls: 0.1804  d0.loss_bbox: 0.1047  d0.loss_iou: 0.1513  d1.loss_cls: 0.1030  d1.loss_bbox: 0.0789  d1.loss_iou: 0.1102  d2.loss_cls: 0.0547  d2.loss_bbox: 0.0683  d2.loss_iou: 0.0974  d3.loss_cls: 0.0409  d3.loss_bbox: 0.0665  d3.loss_iou: 0.0938  d4.loss_cls: 0.0433  d4.loss_bbox: 0.0656  d4.loss_iou: 0.0942  enc_loss_cls: 0.1357  enc_loss_bbox: 0.1767  enc_loss_iou: 0.2421  dn_loss_cls: 0.0293  dn_loss_bbox: 0.0869  dn_loss_iou: 0.1197  d0.dn_loss_cls: 0.0607  d0.dn_loss_bbox: 0.2582  d0.dn_loss_iou: 0.3350  d1.dn_loss_cls: 0.0345  d1.dn_loss_bbox: 0.1234  d1.dn_loss_iou: 0.1625  d2.dn_loss_cls: 0.0285  d2.dn_loss_bbox: 0.0917  d2.dn_loss_iou: 0.1252  d3.dn_loss_cls: 0.0286  d3.dn_loss_bbox: 0.0868  d3.dn_loss_iou: 0.1190  d4.dn_loss_cls: 0.0283  d4.dn_loss_bbox: 0.0868  d4.dn_loss_iou: 0.1193  loss_rpn_cls: 0.0070  loss_rpn_bbox: 0.0561  loss_cls0: 0.6645  acc0: 98.2422  loss_bbox0: 1.3126  loss_cls1: 0.8557  loss_bbox1: 1.8590  loss_centerness1: 6.8963  loss_cls_aux0: 0.0018  loss_bbox_aux0: 0.0393  loss_iou_aux0: 0.0613  d0.loss_cls_aux0: 0.0123  d0.loss_bbox_aux0: 0.1556  d0.loss_iou_aux0: 0.2078  d1.loss_cls_aux0: 0.0102  d1.loss_bbox_aux0: 0.0730  d1.loss_iou_aux0: 0.1019  d2.loss_cls_aux0: 0.0069  d2.loss_bbox_aux0: 0.0454  d2.loss_iou_aux0: 0.0693  d3.loss_cls_aux0: 0.0024  d3.loss_bbox_aux0: 0.0386  d3.loss_iou_aux0: 0.0600  d4.loss_cls_aux0: 0.0015  d4.loss_bbox_aux0: 0.0391  d4.loss_iou_aux0: 0.0607  loss_cls_aux1: 0.0024  loss_bbox_aux1: 0.0545  loss_iou_aux1: 0.0789  d0.loss_cls_aux1: 0.0077  d0.loss_bbox_aux1: 0.1801  d0.loss_iou_aux1: 0.2364  d1.loss_cls_aux1: 0.0053  d1.loss_bbox_aux1: 0.0840  d1.loss_iou_aux1: 0.1153  d2.loss_cls_aux1: 0.0058  d2.loss_bbox_aux1: 0.0586  d2.loss_iou_aux1: 0.0838  d3.loss_cls_aux1: 0.0024  d3.loss_bbox_aux1: 0.0543  d3.loss_iou_aux1: 0.0783  d4.loss_cls_aux1: 0.0019  d4.loss_bbox_aux1: 0.0544  d4.loss_iou_aux1: 0.0785
2023/12/06 21:31:01 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:31:01 - mmengine - INFO - Saving checkpoint at 5 epochs
2023/12/06 21:31:30 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:31:30 - mmengine - INFO - bbox_mAP_copypaste: 0.801 0.928 0.894 0.120 0.671 0.867
2023/12/06 21:31:30 - mmengine - INFO - Epoch(val) [5][13/13]    coco/bbox_mAP: 0.8010  coco/bbox_mAP_50: 0.9280  coco/bbox_mAP_75: 0.8940  coco/bbox_mAP_s: 0.1200  coco/bbox_mAP_m: 0.6710  coco/bbox_mAP_l: 0.8670  data_time: 0.0036  time: 0.6443
2023/12/06 21:33:38 - mmengine - INFO - Epoch(train)  [6][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:17:51  time: 2.5687  data_time: 0.0049  memory: 9614  grad_norm: 171.9358  loss: 18.6212  loss_cls: 0.0537  loss_bbox: 0.0690  loss_iou: 0.0979  d0.loss_cls: 0.1926  d0.loss_bbox: 0.1200  d0.loss_iou: 0.1485  d1.loss_cls: 0.1394  d1.loss_bbox: 0.0797  d1.loss_iou: 0.1095  d2.loss_cls: 0.1128  d2.loss_bbox: 0.0690  d2.loss_iou: 0.0987  d3.loss_cls: 0.0573  d3.loss_bbox: 0.0685  d3.loss_iou: 0.0970  d4.loss_cls: 0.0542  d4.loss_bbox: 0.0689  d4.loss_iou: 0.0974  enc_loss_cls: 0.1386  enc_loss_bbox: 0.2172  enc_loss_iou: 0.2657  dn_loss_cls: 0.0458  dn_loss_bbox: 0.0928  dn_loss_iou: 0.1241  d0.dn_loss_cls: 0.0559  d0.dn_loss_bbox: 0.2611  d0.dn_loss_iou: 0.3193  d1.dn_loss_cls: 0.0423  d1.dn_loss_bbox: 0.1286  d1.dn_loss_iou: 0.1688  d2.dn_loss_cls: 0.0362  d2.dn_loss_bbox: 0.0963  d2.dn_loss_iou: 0.1290  d3.dn_loss_cls: 0.0398  d3.dn_loss_bbox: 0.0921  d3.dn_loss_iou: 0.1232  d4.dn_loss_cls: 0.0416  d4.dn_loss_bbox: 0.0926  d4.dn_loss_iou: 0.1237  loss_rpn_cls: 0.0206  loss_rpn_bbox: 0.0814  loss_cls0: 0.6268  acc0: 94.5312  loss_bbox0: 1.3875  loss_cls1: 0.9706  loss_bbox1: 1.8981  loss_centerness1: 7.0609  loss_cls_aux0: 0.0008  loss_bbox_aux0: 0.0419  loss_iou_aux0: 0.0640  d0.loss_cls_aux0: 0.0044  d0.loss_bbox_aux0: 0.1569  d0.loss_iou_aux0: 0.1908  d1.loss_cls_aux0: 0.0058  d1.loss_bbox_aux0: 0.0693  d1.loss_iou_aux0: 0.0959  d2.loss_cls_aux0: 0.0032  d2.loss_bbox_aux0: 0.0436  d2.loss_iou_aux0: 0.0664  d3.loss_cls_aux0: 0.0011  d3.loss_bbox_aux0: 0.0413  d3.loss_iou_aux0: 0.0625  d4.loss_cls_aux0: 0.0007  d4.loss_bbox_aux0: 0.0417  d4.loss_iou_aux0: 0.0634  loss_cls_aux1: 0.0009  loss_bbox_aux1: 0.0590  loss_iou_aux1: 0.0873  d0.loss_cls_aux1: 0.0040  d0.loss_bbox_aux1: 0.1971  d0.loss_iou_aux1: 0.2380  d1.loss_cls_aux1: 0.0045  d1.loss_bbox_aux1: 0.0915  d1.loss_iou_aux1: 0.1235  d2.loss_cls_aux1: 0.0036  d2.loss_bbox_aux1: 0.0609  d2.loss_iou_aux1: 0.0892  d3.loss_cls_aux1: 0.0014  d3.loss_bbox_aux1: 0.0586  d3.loss_iou_aux1: 0.0866  d4.loss_cls_aux1: 0.0008  d4.loss_bbox_aux1: 0.0589  d4.loss_iou_aux1: 0.0869
2023/12/06 21:34:07 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:34:07 - mmengine - INFO - Saving checkpoint at 6 epochs
2023/12/06 21:34:36 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:34:37 - mmengine - INFO - bbox_mAP_copypaste: 0.800 0.914 0.881 0.055 0.747 0.860
2023/12/06 21:34:37 - mmengine - INFO - Epoch(val) [6][13/13]    coco/bbox_mAP: 0.8000  coco/bbox_mAP_50: 0.9140  coco/bbox_mAP_75: 0.8810  coco/bbox_mAP_s: 0.0550  coco/bbox_mAP_m: 0.7470  coco/bbox_mAP_l: 0.8600  data_time: 0.0036  time: 0.6437
2023/12/06 21:36:55 - mmengine - INFO - Epoch(train)  [7][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:14:53  time: 2.7765  data_time: 0.0047  memory: 10038  grad_norm: 277.2985  loss: 18.5309  loss_cls: 0.1058  loss_bbox: 0.0531  loss_iou: 0.0777  d0.loss_cls: 0.1936  d0.loss_bbox: 0.1007  d0.loss_iou: 0.1341  d1.loss_cls: 0.1156  d1.loss_bbox: 0.0662  d1.loss_iou: 0.0952  d2.loss_cls: 0.0957  d2.loss_bbox: 0.0555  d2.loss_iou: 0.0804  d3.loss_cls: 0.0969  d3.loss_bbox: 0.0525  d3.loss_iou: 0.0767  d4.loss_cls: 0.1013  d4.loss_bbox: 0.0529  d4.loss_iou: 0.0772  enc_loss_cls: 0.1859  enc_loss_bbox: 0.1707  enc_loss_iou: 0.2278  dn_loss_cls: 0.0857  dn_loss_bbox: 0.0718  dn_loss_iou: 0.1003  d0.dn_loss_cls: 0.0660  d0.dn_loss_bbox: 0.2676  d0.dn_loss_iou: 0.3352  d1.dn_loss_cls: 0.0712  d1.dn_loss_bbox: 0.1148  d1.dn_loss_iou: 0.1498  d2.dn_loss_cls: 0.0715  d2.dn_loss_bbox: 0.0820  d2.dn_loss_iou: 0.1106  d3.dn_loss_cls: 0.0810  d3.dn_loss_bbox: 0.0715  d3.dn_loss_iou: 0.0996  d4.dn_loss_cls: 0.0879  d4.dn_loss_bbox: 0.0716  d4.dn_loss_iou: 0.0998  loss_rpn_cls: 0.0249  loss_rpn_bbox: 0.0469  loss_cls0: 0.6923  acc0: 99.0234  loss_bbox0: 1.1880  loss_cls1: 1.0835  loss_bbox1: 1.7134  loss_centerness1: 6.7736  loss_cls_aux0: 0.0916  loss_bbox_aux0: 0.0374  loss_iou_aux0: 0.0538  d0.loss_cls_aux0: 0.0507  d0.loss_bbox_aux0: 0.1599  d0.loss_iou_aux0: 0.2083  d1.loss_cls_aux0: 0.0517  d1.loss_bbox_aux0: 0.0696  d1.loss_iou_aux0: 0.0929  d2.loss_cls_aux0: 0.0427  d2.loss_bbox_aux0: 0.0399  d2.loss_iou_aux0: 0.0577  d3.loss_cls_aux0: 0.0464  d3.loss_bbox_aux0: 0.0367  d3.loss_iou_aux0: 0.0524  d4.loss_cls_aux0: 0.0769  d4.loss_bbox_aux0: 0.0371  d4.loss_iou_aux0: 0.0532  loss_cls_aux1: 0.0760  loss_bbox_aux1: 0.0537  loss_iou_aux1: 0.0788  d0.loss_cls_aux1: 0.0508  d0.loss_bbox_aux1: 0.1799  d0.loss_iou_aux1: 0.2351  d1.loss_cls_aux1: 0.0522  d1.loss_bbox_aux1: 0.0768  d1.loss_iou_aux1: 0.1047  d2.loss_cls_aux1: 0.0693  d2.loss_bbox_aux1: 0.0530  d2.loss_iou_aux1: 0.0749  d3.loss_cls_aux1: 0.0567  d3.loss_bbox_aux1: 0.0538  d3.loss_iou_aux1: 0.0791  d4.loss_cls_aux1: 0.0690  d4.loss_bbox_aux1: 0.0534  d4.loss_iou_aux1: 0.0783
2023/12/06 21:37:25 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:37:25 - mmengine - INFO - Saving checkpoint at 7 epochs
2023/12/06 21:37:55 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:37:55 - mmengine - INFO - bbox_mAP_copypaste: 0.839 0.955 0.913 0.375 0.754 0.883
2023/12/06 21:37:55 - mmengine - INFO - Epoch(val) [7][13/13]    coco/bbox_mAP: 0.8390  coco/bbox_mAP_50: 0.9550  coco/bbox_mAP_75: 0.9130  coco/bbox_mAP_s: 0.3750  coco/bbox_mAP_m: 0.7540  coco/bbox_mAP_l: 0.8830  data_time: 0.0039  time: 0.6442
2023/12/06 21:40:04 - mmengine - INFO - Epoch(train)  [8][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:11:53  time: 2.5740  data_time: 0.0046  memory: 9839  grad_norm: 184.6256  loss: 16.2961  loss_cls: 0.0321  loss_bbox: 0.0589  loss_iou: 0.0997  d0.loss_cls: 0.1258  d0.loss_bbox: 0.0877  d0.loss_iou: 0.1331  d1.loss_cls: 0.0519  d1.loss_bbox: 0.0617  d1.loss_iou: 0.1030  d2.loss_cls: 0.0350  d2.loss_bbox: 0.0575  d2.loss_iou: 0.0961  d3.loss_cls: 0.0268  d3.loss_bbox: 0.0580  d3.loss_iou: 0.0977  d4.loss_cls: 0.0299  d4.loss_bbox: 0.0587  d4.loss_iou: 0.0987  enc_loss_cls: 0.1141  enc_loss_bbox: 0.1904  enc_loss_iou: 0.2588  dn_loss_cls: 0.0247  dn_loss_bbox: 0.0720  dn_loss_iou: 0.1106  d0.dn_loss_cls: 0.0506  d0.dn_loss_bbox: 0.2248  d0.dn_loss_iou: 0.2901  d1.dn_loss_cls: 0.0302  d1.dn_loss_bbox: 0.1006  d1.dn_loss_iou: 0.1385  d2.dn_loss_cls: 0.0266  d2.dn_loss_bbox: 0.0744  d2.dn_loss_iou: 0.1106  d3.dn_loss_cls: 0.0233  d3.dn_loss_bbox: 0.0712  d3.dn_loss_iou: 0.1085  d4.dn_loss_cls: 0.0228  d4.dn_loss_bbox: 0.0717  d4.dn_loss_iou: 0.1096  loss_rpn_cls: 0.0026  loss_rpn_bbox: 0.0686  loss_cls0: 0.4757  acc0: 99.0234  loss_bbox0: 1.1441  loss_cls1: 0.7218  loss_bbox1: 1.5996  loss_centerness1: 6.8478  loss_cls_aux0: 0.0012  loss_bbox_aux0: 0.0333  loss_iou_aux0: 0.0535  d0.loss_cls_aux0: 0.0078  d0.loss_bbox_aux0: 0.1337  d0.loss_iou_aux0: 0.1775  d1.loss_cls_aux0: 0.0032  d1.loss_bbox_aux0: 0.0564  d1.loss_iou_aux0: 0.0832  d2.loss_cls_aux0: 0.0032  d2.loss_bbox_aux0: 0.0366  d2.loss_iou_aux0: 0.0573  d3.loss_cls_aux0: 0.0019  d3.loss_bbox_aux0: 0.0326  d3.loss_iou_aux0: 0.0513  d4.loss_cls_aux0: 0.0012  d4.loss_bbox_aux0: 0.0329  d4.loss_iou_aux0: 0.0524  loss_cls_aux1: 0.0028  loss_bbox_aux1: 0.0518  loss_iou_aux1: 0.0811  d0.loss_cls_aux1: 0.0087  d0.loss_bbox_aux1: 0.1615  d0.loss_iou_aux1: 0.2027  d1.loss_cls_aux1: 0.0076  d1.loss_bbox_aux1: 0.0677  d1.loss_iou_aux1: 0.0911  d2.loss_cls_aux1: 0.0055  d2.loss_bbox_aux1: 0.0526  d2.loss_iou_aux1: 0.0797  d3.loss_cls_aux1: 0.0030  d3.loss_bbox_aux1: 0.0513  d3.loss_iou_aux1: 0.0791  d4.loss_cls_aux1: 0.0023  d4.loss_bbox_aux1: 0.0515  d4.loss_iou_aux1: 0.0801
2023/12/06 21:40:32 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:40:32 - mmengine - INFO - Saving checkpoint at 8 epochs
2023/12/06 21:41:02 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:41:02 - mmengine - INFO - bbox_mAP_copypaste: 0.827 0.933 0.908 0.175 0.754 0.878
2023/12/06 21:41:02 - mmengine - INFO - Epoch(val) [8][13/13]    coco/bbox_mAP: 0.8270  coco/bbox_mAP_50: 0.9330  coco/bbox_mAP_75: 0.9080  coco/bbox_mAP_s: 0.1750  coco/bbox_mAP_m: 0.7540  coco/bbox_mAP_l: 0.8780  data_time: 0.0056  time: 0.6460
2023/12/06 21:43:05 - mmengine - INFO - Epoch(train)  [9][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:08:55  time: 2.4572  data_time: 0.0053  memory: 10382  grad_norm: 180.1240  loss: 17.0019  loss_cls: 0.0232  loss_bbox: 0.0573  loss_iou: 0.1059  d0.loss_cls: 0.1154  d0.loss_bbox: 0.1018  d0.loss_iou: 0.1665  d1.loss_cls: 0.0490  d1.loss_bbox: 0.0642  d1.loss_iou: 0.1151  d2.loss_cls: 0.0309  d2.loss_bbox: 0.0553  d2.loss_iou: 0.1024  d3.loss_cls: 0.0267  d3.loss_bbox: 0.0560  d3.loss_iou: 0.1044  d4.loss_cls: 0.0227  d4.loss_bbox: 0.0559  d4.loss_iou: 0.1047  enc_loss_cls: 0.1023  enc_loss_bbox: 0.2231  enc_loss_iou: 0.3086  dn_loss_cls: 0.0185  dn_loss_bbox: 0.0816  dn_loss_iou: 0.1258  d0.dn_loss_cls: 0.0446  d0.dn_loss_bbox: 0.2317  d0.dn_loss_iou: 0.3236  d1.dn_loss_cls: 0.0265  d1.dn_loss_bbox: 0.1072  d1.dn_loss_iou: 0.1589  d2.dn_loss_cls: 0.0214  d2.dn_loss_bbox: 0.0829  d2.dn_loss_iou: 0.1275  d3.dn_loss_cls: 0.0193  d3.dn_loss_bbox: 0.0819  d3.dn_loss_iou: 0.1251  d4.dn_loss_cls: 0.0188  d4.dn_loss_bbox: 0.0815  d4.dn_loss_iou: 0.1253  loss_rpn_cls: 0.0120  loss_rpn_bbox: 0.0569  loss_cls0: 0.5108  acc0: 98.0469  loss_bbox0: 1.0924  loss_cls1: 0.8390  loss_bbox1: 1.6272  loss_centerness1: 6.9596  loss_cls_aux0: 0.0007  loss_bbox_aux0: 0.0362  loss_iou_aux0: 0.0614  d0.loss_cls_aux0: 0.0042  d0.loss_bbox_aux0: 0.1388  d0.loss_iou_aux0: 0.1929  d1.loss_cls_aux0: 0.0027  d1.loss_bbox_aux0: 0.0590  d1.loss_iou_aux0: 0.0880  d2.loss_cls_aux0: 0.0025  d2.loss_bbox_aux0: 0.0386  d2.loss_iou_aux0: 0.0633  d3.loss_cls_aux0: 0.0013  d3.loss_bbox_aux0: 0.0361  d3.loss_iou_aux0: 0.0600  d4.loss_cls_aux0: 0.0006  d4.loss_bbox_aux0: 0.0360  d4.loss_iou_aux0: 0.0605  loss_cls_aux1: 0.0007  loss_bbox_aux1: 0.0621  loss_iou_aux1: 0.0838  d0.loss_cls_aux1: 0.0057  d0.loss_bbox_aux1: 0.1995  d0.loss_iou_aux1: 0.2354  d1.loss_cls_aux1: 0.0025  d1.loss_bbox_aux1: 0.0845  d1.loss_iou_aux1: 0.1076  d2.loss_cls_aux1: 0.0029  d2.loss_bbox_aux1: 0.0638  d2.loss_iou_aux1: 0.0848  d3.loss_cls_aux1: 0.0018  d3.loss_bbox_aux1: 0.0625  d3.loss_iou_aux1: 0.0834  d4.loss_cls_aux1: 0.0009  d4.loss_bbox_aux1: 0.0620  d4.loss_iou_aux1: 0.0834
2023/12/06 21:43:33 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:43:33 - mmengine - INFO - Saving checkpoint at 9 epochs
2023/12/06 21:44:03 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:44:03 - mmengine - INFO - bbox_mAP_copypaste: 0.810 0.923 0.896 0.204 0.719 0.861
2023/12/06 21:44:03 - mmengine - INFO - Epoch(val) [9][13/13]    coco/bbox_mAP: 0.8100  coco/bbox_mAP_50: 0.9230  coco/bbox_mAP_75: 0.8960  coco/bbox_mAP_s: 0.2040  coco/bbox_mAP_m: 0.7190  coco/bbox_mAP_l: 0.8610  data_time: 0.0100  time: 0.6508
2023/12/06 21:46:12 - mmengine - INFO - Epoch(train) [10][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:06:04  time: 2.5693  data_time: 0.0054  memory: 10389  grad_norm: 189.4202  loss: 18.9584  loss_cls: 0.1445  loss_bbox: 0.0544  loss_iou: 0.0856  d0.loss_cls: 0.1915  d0.loss_bbox: 0.1046  d0.loss_iou: 0.1621  d1.loss_cls: 0.1368  d1.loss_bbox: 0.0625  d1.loss_iou: 0.1047  d2.loss_cls: 0.1288  d2.loss_bbox: 0.0550  d2.loss_iou: 0.0862  d3.loss_cls: 0.1309  d3.loss_bbox: 0.0546  d3.loss_iou: 0.0848  d4.loss_cls: 0.1358  d4.loss_bbox: 0.0547  d4.loss_iou: 0.0853  enc_loss_cls: 0.1769  enc_loss_bbox: 0.2156  enc_loss_iou: 0.2657  dn_loss_cls: 0.0267  dn_loss_bbox: 0.0749  dn_loss_iou: 0.1141  d0.dn_loss_cls: 0.0486  d0.dn_loss_bbox: 0.2397  d0.dn_loss_iou: 0.3225  d1.dn_loss_cls: 0.0294  d1.dn_loss_bbox: 0.1047  d1.dn_loss_iou: 0.1544  d2.dn_loss_cls: 0.0276  d2.dn_loss_bbox: 0.0799  d2.dn_loss_iou: 0.1199  d3.dn_loss_cls: 0.0256  d3.dn_loss_bbox: 0.0742  d3.dn_loss_iou: 0.1130  d4.dn_loss_cls: 0.0290  d4.dn_loss_bbox: 0.0748  d4.dn_loss_iou: 0.1135  loss_rpn_cls: 0.0546  loss_rpn_bbox: 0.0721  loss_cls0: 0.5789  acc0: 99.2188  loss_bbox0: 1.1361  loss_cls1: 1.3233  loss_bbox1: 1.8461  loss_centerness1: 6.8390  loss_cls_aux0: 0.0846  loss_bbox_aux0: 0.0410  loss_iou_aux0: 0.0677  d0.loss_cls_aux0: 0.0492  d0.loss_bbox_aux0: 0.1465  d0.loss_iou_aux0: 0.1998  d1.loss_cls_aux0: 0.0345  d1.loss_bbox_aux0: 0.0682  d1.loss_iou_aux0: 0.1073  d2.loss_cls_aux0: 0.0577  d2.loss_bbox_aux0: 0.0441  d2.loss_iou_aux0: 0.0735  d3.loss_cls_aux0: 0.0686  d3.loss_bbox_aux0: 0.0406  d3.loss_iou_aux0: 0.0664  d4.loss_cls_aux0: 0.0745  d4.loss_bbox_aux0: 0.0408  d4.loss_iou_aux0: 0.0672  loss_cls_aux1: 0.0285  loss_bbox_aux1: 0.0678  loss_iou_aux1: 0.1009  d0.loss_cls_aux1: 0.0288  d0.loss_bbox_aux1: 0.1966  d0.loss_iou_aux1: 0.2458  d1.loss_cls_aux1: 0.0181  d1.loss_bbox_aux1: 0.0924  d1.loss_iou_aux1: 0.1264  d2.loss_cls_aux1: 0.0188  d2.loss_bbox_aux1: 0.0732  d2.loss_iou_aux1: 0.1033  d3.loss_cls_aux1: 0.0222  d3.loss_bbox_aux1: 0.0672  d3.loss_iou_aux1: 0.0994  d4.loss_cls_aux1: 0.0254  d4.loss_bbox_aux1: 0.0676  d4.loss_iou_aux1: 0.1003
2023/12/06 21:46:35 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:46:35 - mmengine - INFO - Saving checkpoint at 10 epochs
2023/12/06 21:47:07 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:47:07 - mmengine - INFO - bbox_mAP_copypaste: 0.824 0.948 0.905 0.208 0.781 0.865
2023/12/06 21:47:07 - mmengine - INFO - Epoch(val) [10][13/13]    coco/bbox_mAP: 0.8240  coco/bbox_mAP_50: 0.9480  coco/bbox_mAP_75: 0.9050  coco/bbox_mAP_s: 0.2080  coco/bbox_mAP_m: 0.7810  coco/bbox_mAP_l: 0.8650  data_time: 0.0141  time: 0.6546
2023/12/06 21:49:16 - mmengine - INFO - Epoch(train) [11][50/61]  base_lr: 2.0000e-04 lr: 2.0000e-05  eta: 0:03:15  time: 2.5813  data_time: 0.0054  memory: 9725  grad_norm: 161.1662  loss: 15.5229  loss_cls: 0.0370  loss_bbox: 0.0527  loss_iou: 0.0781  d0.loss_cls: 0.1395  d0.loss_bbox: 0.0808  d0.loss_iou: 0.1115  d1.loss_cls: 0.0616  d1.loss_bbox: 0.0604  d1.loss_iou: 0.0860  d2.loss_cls: 0.0466  d2.loss_bbox: 0.0534  d2.loss_iou: 0.0771  d3.loss_cls: 0.0348  d3.loss_bbox: 0.0518  d3.loss_iou: 0.0769  d4.loss_cls: 0.0355  d4.loss_bbox: 0.0522  d4.loss_iou: 0.0772  enc_loss_cls: 0.0863  enc_loss_bbox: 0.1709  enc_loss_iou: 0.2260  dn_loss_cls: 0.0240  dn_loss_bbox: 0.0702  dn_loss_iou: 0.0934  d0.dn_loss_cls: 0.0434  d0.dn_loss_bbox: 0.1976  d0.dn_loss_iou: 0.2528  d1.dn_loss_cls: 0.0254  d1.dn_loss_bbox: 0.0965  d1.dn_loss_iou: 0.1240  d2.dn_loss_cls: 0.0197  d2.dn_loss_bbox: 0.0731  d2.dn_loss_iou: 0.0965  d3.dn_loss_cls: 0.0206  d3.dn_loss_bbox: 0.0700  d3.dn_loss_iou: 0.0933  d4.dn_loss_cls: 0.0213  d4.dn_loss_bbox: 0.0700  d4.dn_loss_iou: 0.0931  loss_rpn_cls: 0.0030  loss_rpn_bbox: 0.0476  loss_cls0: 0.4202  acc0: 99.2188  loss_bbox0: 0.9226  loss_cls1: 0.7097  loss_bbox1: 1.5417  loss_centerness1: 7.0162  loss_cls_aux0: 0.0003  loss_bbox_aux0: 0.0317  loss_iou_aux0: 0.0482  d0.loss_cls_aux0: 0.0015  d0.loss_bbox_aux0: 0.1155  d0.loss_iou_aux0: 0.1523  d1.loss_cls_aux0: 0.0014  d1.loss_bbox_aux0: 0.0492  d1.loss_iou_aux0: 0.0723  d2.loss_cls_aux0: 0.0010  d2.loss_bbox_aux0: 0.0346  d2.loss_iou_aux0: 0.0522  d3.loss_cls_aux0: 0.0006  d3.loss_bbox_aux0: 0.0311  d3.loss_iou_aux0: 0.0469  d4.loss_cls_aux0: 0.0004  d4.loss_bbox_aux0: 0.0315  d4.loss_iou_aux0: 0.0477  loss_cls_aux1: 0.0003  loss_bbox_aux1: 0.0497  loss_iou_aux1: 0.0730  d0.loss_cls_aux1: 0.0011  d0.loss_bbox_aux1: 0.1352  d0.loss_iou_aux1: 0.1742  d1.loss_cls_aux1: 0.0010  d1.loss_bbox_aux1: 0.0656  d1.loss_iou_aux1: 0.0881  d2.loss_cls_aux1: 0.0013  d2.loss_bbox_aux1: 0.0527  d2.loss_iou_aux1: 0.0751  d3.loss_cls_aux1: 0.0008  d3.loss_bbox_aux1: 0.0494  d3.loss_iou_aux1: 0.0726  d4.loss_cls_aux1: 0.0004  d4.loss_bbox_aux1: 0.0494  d4.loss_iou_aux1: 0.0728
2023/12/06 21:49:42 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:49:42 - mmengine - INFO - Saving checkpoint at 11 epochs
2023/12/06 21:50:14 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:50:14 - mmengine - INFO - bbox_mAP_copypaste: 0.818 0.947 0.914 0.262 0.794 0.851
2023/12/06 21:50:14 - mmengine - INFO - Epoch(val) [11][13/13]    coco/bbox_mAP: 0.8180  coco/bbox_mAP_50: 0.9470  coco/bbox_mAP_75: 0.9140  coco/bbox_mAP_s: 0.2620  coco/bbox_mAP_m: 0.7940  coco/bbox_mAP_l: 0.8510  data_time: 0.0369  time: 0.6770
2023/12/06 21:52:17 - mmengine - INFO - Epoch(train) [12][50/61]  base_lr: 2.0000e-05 lr: 2.0000e-06  eta: 0:00:29  time: 2.4456  data_time: 0.0054  memory: 10672  grad_norm: 147.7582  loss: 15.2347  loss_cls: 0.0209  loss_bbox: 0.0514  loss_iou: 0.0811  d0.loss_cls: 0.1556  d0.loss_bbox: 0.0752  d0.loss_iou: 0.1144  d1.loss_cls: 0.0595  d1.loss_bbox: 0.0518  d1.loss_iou: 0.0882  d2.loss_cls: 0.0379  d2.loss_bbox: 0.0495  d2.loss_iou: 0.0796  d3.loss_cls: 0.0219  d3.loss_bbox: 0.0514  d3.loss_iou: 0.0812  d4.loss_cls: 0.0203  d4.loss_bbox: 0.0517  d4.loss_iou: 0.0814  enc_loss_cls: 0.1071  enc_loss_bbox: 0.1414  enc_loss_iou: 0.2026  dn_loss_cls: 0.0183  dn_loss_bbox: 0.0712  dn_loss_iou: 0.1025  d0.dn_loss_cls: 0.0424  d0.dn_loss_bbox: 0.1922  d0.dn_loss_iou: 0.2540  d1.dn_loss_cls: 0.0219  d1.dn_loss_bbox: 0.0901  d1.dn_loss_iou: 0.1262  d2.dn_loss_cls: 0.0185  d2.dn_loss_bbox: 0.0728  d2.dn_loss_iou: 0.1046  d3.dn_loss_cls: 0.0185  d3.dn_loss_bbox: 0.0711  d3.dn_loss_iou: 0.1023  d4.dn_loss_cls: 0.0179  d4.dn_loss_bbox: 0.0712  d4.dn_loss_iou: 0.1024  loss_rpn_cls: 0.0038  loss_rpn_bbox: 0.0489  loss_cls0: 0.3972  acc0: 98.6328  loss_bbox0: 0.9320  loss_cls1: 0.6266  loss_bbox1: 1.4479  loss_centerness1: 7.0397  loss_cls_aux0: 0.0009  loss_bbox_aux0: 0.0271  loss_iou_aux0: 0.0445  d0.loss_cls_aux0: 0.0028  d0.loss_bbox_aux0: 0.1070  d0.loss_iou_aux0: 0.1487  d1.loss_cls_aux0: 0.0020  d1.loss_bbox_aux0: 0.0467  d1.loss_iou_aux0: 0.0697  d2.loss_cls_aux0: 0.0014  d2.loss_bbox_aux0: 0.0295  d2.loss_iou_aux0: 0.0480  d3.loss_cls_aux0: 0.0011  d3.loss_bbox_aux0: 0.0272  d3.loss_iou_aux0: 0.0438  d4.loss_cls_aux0: 0.0009  d4.loss_bbox_aux0: 0.0271  d4.loss_iou_aux0: 0.0442  loss_cls_aux1: 0.0010  loss_bbox_aux1: 0.0475  loss_iou_aux1: 0.0743  d0.loss_cls_aux1: 0.0023  d0.loss_bbox_aux1: 0.1291  d0.loss_iou_aux1: 0.1750  d1.loss_cls_aux1: 0.0016  d1.loss_bbox_aux1: 0.0576  d1.loss_iou_aux1: 0.0875  d2.loss_cls_aux1: 0.0012  d2.loss_bbox_aux1: 0.0475  d2.loss_iou_aux1: 0.0739  d3.loss_cls_aux1: 0.0010  d3.loss_bbox_aux1: 0.0475  d3.loss_iou_aux1: 0.0742  d4.loss_cls_aux1: 0.0008  d4.loss_bbox_aux1: 0.0474  d4.loss_iou_aux1: 0.0742
2023/12/06 21:52:45 - mmengine - INFO - Exp name: co_dino_5scale_swin_l_16xb1_1x_balloon_20231206_211246
2023/12/06 21:52:45 - mmengine - INFO - Saving checkpoint at 12 epochs
2023/12/06 21:53:17 - mmengine - INFO - Evaluating bbox...
2023/12/06 21:53:17 - mmengine - INFO - bbox_mAP_copypaste: 0.836 0.933 0.896 0.186 0.820 0.871
2023/12/06 21:53:17 - mmengine - INFO - Epoch(val) [12][13/13]    coco/bbox_mAP: 0.8360  coco/bbox_mAP_50: 0.9330  coco/bbox_mAP_75: 0.8960  coco/bbox_mAP_s: 0.1860  coco/bbox_mAP_m: 0.8200  coco/bbox_mAP_l: 0.8710  data_time: 0.0274  time: 0.6675
